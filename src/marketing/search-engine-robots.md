---
title: Search Engine Robots
redirect_to:
  - https://experienceleague.adobe.com/docs/commerce-admin/marketing/seo/seo-overview.html#search-engine-robots
---

The Commerce configuration includes settings to generate and manage instructions for web crawlers and bots that index your site. If the request for `robots.txt` reaches Commerce (rather than a physical file) it is dynamically routed to the robots controller. The instructions are directives that are recognized and followed by most search engines.

By default, the robots.txt file that is generated by Commerce contains instructions for web crawler to avoid indexing certain parts of the site that contain files that are used internally by the system. You can use the default settings, or define your own custom instructions for all, or for specific search engines. There are many articles online that explore the subject in detail.

## Example: Custom Instructions

### Allows Full Access

    User-agent:*
    Disallow:

### Disallows Access to All Folders

    User-agent:*
    Disallow: /

### Default Instructions

    User-agent: *
    Disallow: /index.php/
    Disallow: /*?
    Disallow: /checkout/
    Disallow: /app/
    Disallow: /lib/
    Disallow: /*.php$
    Disallow: /pkginfo/
    Disallow: /report/
    Disallow: /var/
    Disallow: /catalog/
    Disallow: /customer/
    Disallow: /sendfriend/
    Disallow: /review/
    Disallow: /*SID=

## Configure `robots.txt`

1. On the _Admin_ sidebar, go to **Content** > _Design_ > **Configuration**.

1. Find the **Global** configuration in the first row of the grid and click **Edit**.

    ![GLobal design configuration]({% link marketing/assets/design-configuration-grid.png %}){: .zoom}
    _Global Design Configuration_

1. Scroll down and expand ![Expansion selector]({% link assets/icon-display-expand.png %}) the **Search Engine Robots** section and do the following:

    ![Design configuration - search engine robots]({% link marketing/assets/design-configuration-search-engine-robots.png %}){: .zoom}
    _Search Engine Robots_

    - Set **Default Robots** to one of the following:

        |INDEX, FOLLOW|Instructs web crawlers to index the site and to check back later for changes.|
        |NOINDEX, FOLLOW|Instructs web crawlers to avoid indexing the site, but to check back later for changes.|
        |INDEX, NOFOLLOW|Instructs web crawlers to index the site once, but to not check back later for changes.|
        |NOINDEX, NOFOLLOW|Instructs web crawlers to avoid indexing the site, and to not check back later for changes.|

    - If needed, enter custom instructions into the **Edit Custom instruction of robots.txt file** box. For example, while a site is in development, you might want to disallow access to all folders.

    - To restore the default instructions, click <span class="btn">Reset to Default</span>.

1. When complete, click <span class="btn"> Save Configuration</span>.
